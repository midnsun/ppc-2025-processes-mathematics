## Министерство науки и высшего образования Российской Федерации  
## Федеральное государственное автономное образовательное учреждение высшего образования  
## **«Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского»**  
### Институт информационных технологий, математики и механики  

### **Направление подготовки:** «Прикладная математика и информатика»  

---

# Отчёт  
### По задаче  
**Нахождение максимальных значений по столбцам матрицы**  
**Вариант №16**

**Выполнил:**  
Студент группы 3823Б1ПМоп3  
**Загрядсков М.А.**

**Преподаватель:**  
доцент **Сысоев А.В.**

**Нижний Новгород, 2025**

---

## Введение

В данной работе реализован и исследован алгоритм нахождения максимальных значений по каждому столбцу матрицы. Задача решается в последовательной и в параллельной форме с использованием технологии MPI (Message Passing Interface). Параллельная реализация направлена на сокращение времени вычислений за счёт распределения данных по процессам.

---

## Постановка задачи

Необходимо для заданной матрицы, представленной в виде одномерного массива (вектор `std::vector<double>`), найти максимальные значения по каждому из её столбцов.  

Пусть матрица имеет размерность **m × n**, где  
- *m* — число строк,  
- *n* — число столбцов.  

Элементы матрицы хранятся **по столбцам**, то есть элемент с индексом *(i, j)* находится в массиве под индексом `j * m + i`.

Требуется:  
1. Реализовать последовательный алгоритм нахождения максимальных элементов по столбцам.  
2. Разработать параллельную реализацию с использованием **MPI**, распределяющую вычисления между несколькими процессами.  
3. Проверить корректность вычислений и провести экспериментальные замеры времени выполнения.  

---

## Описание алгоритма

**Последовательный алгоритм** выполняет следующие шаги:

1. Проверяет корректность входных данных.  
2. Инициализирует вектор результатов значением, минимально возможным для данного типа.  
3. Для каждого столбца `j`:
   - проходится по всем элементам строк `i`,
   - сравнивает текущее значение с максимумом столбца,
   - обновляет максимум при необходимости.  
4. Возвращает вектор максимальных значений по столбцам.

Псевдокод:  
```
for j in 0..n-1:
max_j = -∞ 
for i in 0..m-1:  
if A[i,j] > max_j:  
max_j = A[i,j]  
result[j] = max_j 
```

---

## Описание схемы параллельного алгоритма

Параллельная версия использует **распределение столбцов** между MPI-процессами.  

1. Корневой процесс (`rank 0`) распределяет блоки столбцов между процессами через `MPI_Scatter`.  
2. Каждый процесс вычисляет локальные максимумы по выделенным столбцам.  
3. Локальные результаты собираются на корневом процессе с помощью `MPI_Gather`.  
4. Корневой процесс обрабатывает оставшиеся столбцы (если их число не делится на количество процессов).  
5. Результат рассылается всем процессам через `MPI_Bcast` для обеспечения корректного прохождения функциональных тестов всеми процессами.  

Таким образом, каждый процесс работает с собственной частью данных, что позволяет достичь ускорения при достаточно больших размерах матриц.

---

## Описание MPI-версии

Программная реализация использует тип входных данных в виде кортежа:  
`std::tuple<size_t, std::vector<double>>`, 
где  
- первый элемент — количество столбцов `n`,  
- второй — вектор значений матрицы, хранящийся по столбцам.  

MPI-реализация автоматически определяет используемый тип данных (`MPI_DOUBLE`, `MPI_INT` и т.д.), производит рассылку данных и сбор частичных результатов. Используется синхронизация через `MPI_Barrier`.

---

## Результаты экспериментов и подтверждение корректности

Эксперименты проводились на локальной машине.  
Параметры тестовой матрицы:
- размер **2048 × 4096**.
- элементы типа `double`.
- использовался генератор псевдослучайных чисел `mt19937`, также известный известный как **«Вихрь Мерсенна»**.
- Диапазон значений элементов матрицы: от -10<sup>6</sup> до 10<sup>6</sup>.


| Версия алгоритма | Время выполнения (с) |
|------------------:|---------------------:|
| Последовательная | 0.053 |
| Параллельная (MPI) | 0.019 |

**Подтверждение корректности:**  
Функция тестирования проверяет, что для каждого столбца найденный максимум не меньше всех элементов столбца.  
Все функциональные тесты и тесты производительности были успешно пройдены на локальной машине.

---

## Выводы из результатов

Реализация с использованием MPI показывает ускорение примерно в **2.8 раза** по сравнению с последовательной версией.  
Это демонстрирует эффективность параллельного подхода при работе с крупными матрицами.  
При увеличении числа процессов можно ожидать дальнейшего сокращения времени выполнения, однако при малых размерах матриц затраты на коммуникацию могут нивелировать прирост производительности.

---

## Заключение

В работе реализованы и протестированы последовательная и параллельная (MPI) версии алгоритма нахождения максимальных значений по столбцам матрицы.  
Проведённые эксперименты подтвердили корректность и эффективность параллельной реализации.  

---

## Список литературы

1. Документация в формате веб-сайта по реализации **MPICH** стандарта **MPI**: [https://www.mpich.org](https://www.mpich.org)

---

## Приложение

### Параллельная реализация

```cpp
bool ZagryadskovMMaxByColumnMPI::RunImpl() {
  bool ifDividable = std::get<1>(GetInput()).size() % std::get<0>(GetInput()) == 0;
  bool testData = (std::get<0>(GetInput()) > 0) && (std::get<1>(GetInput()).size() > 0) && ifDividable;
  if (!testData) {
    return false;
  }

  int world_size = 0, world_rank = 0;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  const auto &n = std::get<0>(GetInput());
  const auto &mat = std::get<1>(GetInput());
  size_t m = mat.size() / n;
  OutType &res = GetOutput();
  OutType local_res;
  OutType columns;
  int columns_count = int(n) / world_size;
  int columns_size = columns_count * int(m);
  using T = std::decay_t<decltype(*mat.begin())>;
  MPI_Datatype datatype;
  if (std::is_same<T, char>::value) {
    datatype = MPI_CHAR;
  } else if (std::is_same<T, unsigned char>::value) {
    datatype = MPI_UNSIGNED_CHAR;
  } else if (std::is_same<T, short>::value) {
    datatype = MPI_SHORT;
  } else if (std::is_same<T, unsigned short>::value) {
    datatype = MPI_UNSIGNED_SHORT;
  } else if (std::is_same<T, int>::value) {
    datatype = MPI_INT;
  } else if (std::is_same<T, unsigned>::value) {
    datatype = MPI_UNSIGNED;
  } else if (std::is_same<T, long>::value) {
    datatype = MPI_LONG;
  } else if (std::is_same<T, unsigned long>::value) {
    datatype = MPI_UNSIGNED_LONG;
  } else if (std::is_same<T, long long>::value) {
    datatype = MPI_LONG_LONG;
  } else if (std::is_same<T, float>::value) {
    datatype = MPI_FLOAT;
  } else if (std::is_same<T, double>::value) {
    datatype = MPI_DOUBLE;
  } else {
    return false;
  }

  columns.resize(columns_size);

  res.resize(n, std::numeric_limits<T>::lowest());
  local_res.resize(columns_count, std::numeric_limits<T>::lowest());
  MPI_Scatter(mat.data(), columns_size, datatype, columns.data(), columns_size, datatype, 0, MPI_COMM_WORLD);

  size_t i, j;
  T tmp;
  int tmpFlag;
  for (j = 0; j < size_t(columns_count); ++j) {
    for (i = 0; i < m; ++i) {
      tmp = columns[j * m + i];
      tmpFlag = tmp > local_res[j];
      local_res[j] = tmpFlag * tmp + (!tmpFlag) * local_res[j];
    }
  }
  MPI_Gather(local_res.data(), columns_count, datatype, res.data(), columns_count, datatype, 0, MPI_COMM_WORLD);
  if (world_rank == 0) {
    for (j = size_t(columns_count * world_size); j < n; ++j) {
      for (i = 0; i < m; ++i) {
        tmp = mat[j * m + i];
        tmpFlag = tmp > res[j];
        res[j] = tmpFlag * tmp + (!tmpFlag) * res[j];
      }
    }
  }

  MPI_Bcast(res.data(), res.size(), datatype, 0, MPI_COMM_WORLD);
  MPI_Barrier(MPI_COMM_WORLD);
  return GetOutput().size() > 0;
}
```
